{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from src.helpers.functions import get_data, pc_T, predict_pca, estimate_AR_res, generate_data\n",
    "from src.helpers.functions import select_AR_lag_SIC, winsor, lag_matrix\n",
    "from src.helpers.refactored import ar_forecast, scale_X, reduce_dimensions, loocv_ts, standardize, forecast\n",
    "from src.helpers.autoencoder import Autoencoder\n",
    "from src.helpers.lstm_ae import LSTMAutoencoder\n",
    "from src.helpers.forecast import Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_sample(X, y, dim_method=\"ae\", scale_method=\"distance_correlation\", h=1, hyper_params_grid=None, forecast_method=\"ols\",forecast_params=None):\n",
    "    \"\"\" Function to perform out of sample forecasting \"\"\"\n",
    "    T = y.shape[0]\n",
    "\n",
    "    M = (1984-1959)*12  # In sample periods\n",
    "    N = T - M  # Out of sample periods\n",
    "\n",
    "    forecast_spca = np.zeros((N, 1))  # Forecast errors of scaled PCA\n",
    "    forecast_ar = np.zeros((N, 1))  # Forecast errors of AR model\n",
    "    actual_y = np.zeros((N, 1))  # Actual values of y\n",
    "\n",
    "    # Initialize the models\n",
    "    fc = Forecast(method=forecast_method, hyper_params=forecast_params, h=h)\n",
    "    ae = None\n",
    "\n",
    "    p_max = 3  # Max number of lags for AR(p) model\n",
    "    \n",
    "    # Loop over all out of sample periods\n",
    "    for n in range(N):\n",
    "        # Print every 20 percent\n",
    "        if n % (N // 5) == 0:\n",
    "            print(f\"Out of sample period {n} out of {N} periods\")\n",
    "            \n",
    "        # Use all available data up to time t\n",
    "        X_t = X[:(M + n), :]\n",
    "        y_t = y[:M + n]\n",
    "        actual_y[n] = y[M + n]\n",
    "\n",
    "        # Standardize the data\n",
    "        X_t = standardize(X_t)\n",
    "\n",
    "        # Get number lags\n",
    "        p_AR_star_n = select_AR_lag_SIC(y_t, h, p_max=p_max)\n",
    "\n",
    "        # Compute the forecast of the AR model\n",
    "        forecast_ar[n] = ar_forecast(p_AR_star_n, y_t, h)\n",
    "        \n",
    "        #### STEP 1: Scaling factors ####\n",
    "\n",
    "        # Compute the betas for scaling the variables\n",
    "        beta = scale_X(X_t, y_t, h, method=scale_method, p_AR_star_n=p_AR_star_n)\n",
    "        \n",
    "        # Winsorizing the betas\n",
    "        beta_win = winsor(np.abs(beta), p=(0, 90))\n",
    "\n",
    "        # Scale the factors by the winsorized betas\n",
    "        scaleX_t = X_t * beta_win\n",
    "\n",
    "        #### Intermezzo: Find the optimal number of factors (or other hyperparameters) ####\n",
    "        \n",
    "        if n == 0:\n",
    "            #print(\"Starting hyperparameter optimization\")\n",
    "            hyper_params = loocv_ts(X=X_t, y=y_t, h=h, p_AR_star_n=p_AR_star_n, method=dim_method, scale_method=scale_method, grid=hyper_params_grid)\n",
    "            #print(\"Optimal Dimension Reduction hyperparameters found\")\n",
    "\n",
    "            if dim_method == \"ae\":\n",
    "                # Initialize the autoencoder\n",
    "                ae = Autoencoder(input_dim=X.shape[1], activation=nn.SiLU, hyper_params=hyper_params)\n",
    "\n",
    "                # Train the autoencoder on the in sample data\n",
    "                ae.train_model(scaleX_t, lr=hyper_params.get(\"lr\", 0.001), num_epochs=hyper_params.get(\"epochs\", 300))\n",
    "            elif dim_method == \"lstm\":\n",
    "                # Initialize the autoencoder\n",
    "                ae = LSTMAutoencoder(input_dim=X.shape[1], hyper_params=hyper_params)\n",
    "\n",
    "                # Train the autoencoder on the in sample data\n",
    "                ae.train_model(scaleX_t, lr=hyper_params.get(\"lr\", 0.001), num_epochs=hyper_params.get(\"epochs\", 300))\n",
    "\n",
    "                print(\"Autoencoder training done\")\n",
    "\n",
    "        ### Updating Hyperparameters durign forecasting ###\n",
    "        if n % 60 == 0 and n > 0:\n",
    "            if dim_method == \"ae\":\n",
    "                pass\n",
    "            else:\n",
    "                hyper_params = loocv_ts(X=X_t, y=y_t, h=h, p_AR_star_n=p_AR_star_n, method=dim_method, scale_method=scale_method, grid=hyper_params_grid)\n",
    "            \n",
    "        #### STEP 2: Dimension Reduction ####            \n",
    "\n",
    "        # Compute the reduced dimensionality representation of the factors\n",
    "        x_spc = reduce_dimensions(X=scaleX_t, hyper_params=hyper_params, method=dim_method, dim_red_model=ae)\n",
    "\n",
    "        #### STEP 3: Forecasting ####\n",
    "\n",
    "        # Add lag of y_t to the factors\n",
    "        if p_AR_star_n > 0:\n",
    "            x_spc = lag_matrix(x_spc, y_t, p_AR_star_n)\n",
    "            y_t = y_t[p_AR_star_n-1:]\n",
    "                \n",
    "        # Cross validate the hyperparameters once in first period\n",
    "        if n == 0 and forecast_params:\n",
    "            fc.cross_validate(x_spc, y_t, hyper_params=forecast_params)\n",
    "        elif n > 0 and forecast_params:\n",
    "            if n % 60 == 0:\n",
    "                fc.cross_validate(x_spc, y_t, hyper_params=forecast_params)\n",
    "\n",
    "        # Compute the forecast of the PCA and scaled PCA model\n",
    "        forecast_spca[n] = fc.predict(x_spc, y_t)\n",
    "\n",
    "    # Compute the forecast errors\n",
    "    error_spca = actual_y - forecast_spca\n",
    "    error_ar = actual_y - forecast_ar\n",
    "    \n",
    "    # Compute the R squared out of sample against the AR model\n",
    "    SSE_spca = np.sum(error_spca**2)\n",
    "    SSE_ar = np.sum(error_ar**2)\n",
    "\n",
    "    R2_spca = (1 - SSE_spca / SSE_ar)\n",
    "\n",
    "    print(\"R2_spca: \", round(R2_spca * 100, 2))\n",
    "    \n",
    "    return {\"error_spca\": error_spca,\n",
    "            \"error_ar\": error_ar,\n",
    "            \"R2_spca\": R2_spca,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X:  (720, 123)\n"
     ]
    }
   ],
   "source": [
    "variables = get_data()\n",
    "X = variables['data'].values\n",
    "\n",
    "print(\"Shape of X: \", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:  inflation\n",
      "Out of sample period 0 out of 420 periods\n",
      "Number of model configurations:  16\n",
      "Best model configuration:  {'nfac': 2}\n",
      "Number of model configurations:  16\n",
      "Best model configuration:  {'nfac': 2}\n",
      "Out of sample period 84 out of 420 periods\n",
      "Number of model configurations:  16\n",
      "Best model configuration:  {'nfac': 2}\n",
      "Out of sample period 168 out of 420 periods\n",
      "Number of model configurations:  16\n",
      "Best model configuration:  {'nfac': 2}\n",
      "Number of model configurations:  16\n",
      "Best model configuration:  {'nfac': 2}\n",
      "Out of sample period 252 out of 420 periods\n",
      "Number of model configurations:  16\n",
      "Best model configuration:  {'nfac': 20}\n",
      "Out of sample period 336 out of 420 periods\n",
      "Number of model configurations:  16\n",
      "Best model configuration:  {'nfac': 20}\n",
      "R2_spca:  -26.39\n",
      "Target:  unemployment\n",
      "Out of sample period 0 out of 420 periods\n",
      "Number of model configurations:  16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTarget: \u001b[39m\u001b[39m\"\u001b[39m, target)\n\u001b[0;32m     40\u001b[0m y \u001b[39m=\u001b[39m variables[target]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m---> 41\u001b[0m result \u001b[39m=\u001b[39m out_sample(\n\u001b[0;32m     42\u001b[0m     X \u001b[39m=\u001b[39;49m X,\n\u001b[0;32m     43\u001b[0m     y \u001b[39m=\u001b[39;49m y,\n\u001b[0;32m     44\u001b[0m     scale_method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mregression\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     45\u001b[0m     dim_method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpca\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     46\u001b[0m     forecast_method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mols\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     47\u001b[0m     hyper_params_grid\u001b[39m=\u001b[39;49mpca_params,\n\u001b[0;32m     48\u001b[0m     h\u001b[39m=\u001b[39;49m\u001b[39m12\u001b[39;49m,\n\u001b[0;32m     49\u001b[0m     forecast_params\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,)\n",
      "Cell \u001b[1;32mIn[2], line 53\u001b[0m, in \u001b[0;36mout_sample\u001b[1;34m(X, y, dim_method, scale_method, h, hyper_params_grid, forecast_method, forecast_params)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39m#### Intermezzo: Find the optimal number of factors (or other hyperparameters) ####\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     52\u001b[0m     \u001b[39m#print(\"Starting hyperparameter optimization\")\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     hyper_params \u001b[39m=\u001b[39m loocv_ts(X\u001b[39m=\u001b[39;49mX_t, y\u001b[39m=\u001b[39;49my_t, h\u001b[39m=\u001b[39;49mh, p_AR_star_n\u001b[39m=\u001b[39;49mp_AR_star_n, method\u001b[39m=\u001b[39;49mdim_method, scale_method\u001b[39m=\u001b[39;49mscale_method, grid\u001b[39m=\u001b[39;49mhyper_params_grid)\n\u001b[0;32m     54\u001b[0m     \u001b[39m#print(\"Optimal Dimension Reduction hyperparameters found\")\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m dim_method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mae\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     57\u001b[0m         \u001b[39m# Initialize the autoencoder\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\PythonProjects\\Thesis\\src\\helpers\\refactored.py:179\u001b[0m, in \u001b[0;36mloocv_ts\u001b[1;34m(X, y, h, p_AR_star_n, method, scale_method, grid)\u001b[0m\n\u001b[0;32m    176\u001b[0m y_actual[i, \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m y[window \u001b[39m+\u001b[39m i]\n\u001b[0;32m    178\u001b[0m \u001b[39m# Scale the data\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m scaling_factors \u001b[39m=\u001b[39m scale_X(X_t, y_t, h, p_AR_star_n, scale_method)\n\u001b[0;32m    180\u001b[0m X_t \u001b[39m=\u001b[39m X_t \u001b[39m*\u001b[39m scaling_factors\n\u001b[0;32m    182\u001b[0m \u001b[39m# Reduce the dimensions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\PythonProjects\\Thesis\\src\\helpers\\refactored.py:42\u001b[0m, in \u001b[0;36mscale_X\u001b[1;34m(X_t, y_t, h, p_AR_star_n, method, model)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[39m# Compute the betas for scaling the variables\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(X_t\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[1;32m---> 42\u001b[0m         lr\u001b[39m.\u001b[39;49mfit(X_t[:\u001b[39m-\u001b[39;49mh, j]\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m), y_t[h:])\n\u001b[0;32m     43\u001b[0m         scaling_factors[j] \u001b[39m=\u001b[39m lr\u001b[39m.\u001b[39mcoef_[\u001b[39m0\u001b[39m]\n\u001b[0;32m     44\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdistance_correlation\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_base.py:665\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    656\u001b[0m X, y, X_offset, y_offset, X_scale \u001b[39m=\u001b[39m _preprocess_data(\n\u001b[0;32m    657\u001b[0m     X,\n\u001b[0;32m    658\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    661\u001b[0m     sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[0;32m    662\u001b[0m )\n\u001b[0;32m    664\u001b[0m \u001b[39m# Sample weight can be implemented via a simple rescaling.\u001b[39;00m\n\u001b[1;32m--> 665\u001b[0m X, y, sample_weight_sqrt \u001b[39m=\u001b[39m _rescale_data(X, y, sample_weight)\n\u001b[0;32m    667\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositive:\n\u001b[0;32m    668\u001b[0m     \u001b[39mif\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_base.py:322\u001b[0m, in \u001b[0;36m_rescale_data\u001b[1;34m(X, y, sample_weight)\u001b[0m\n\u001b[0;32m    320\u001b[0m sample_weight_sqrt \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(sample_weight)\n\u001b[0;32m    321\u001b[0m sw_matrix \u001b[39m=\u001b[39m sparse\u001b[39m.\u001b[39mdia_matrix((sample_weight_sqrt, \u001b[39m0\u001b[39m), shape\u001b[39m=\u001b[39m(n_samples, n_samples))\n\u001b[1;32m--> 322\u001b[0m X \u001b[39m=\u001b[39m safe_sparse_dot(sw_matrix, X)\n\u001b[0;32m    323\u001b[0m y \u001b[39m=\u001b[39m safe_sparse_dot(sw_matrix, y)\n\u001b[0;32m    324\u001b[0m \u001b[39mreturn\u001b[39;00m X, y, sample_weight_sqrt\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\sklearn\\utils\\extmath.py:189\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    187\u001b[0m         ret \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(a, b)\n\u001b[0;32m    188\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 189\u001b[0m     ret \u001b[39m=\u001b[39m a \u001b[39m@\u001b[39;49m b\n\u001b[0;32m    191\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    192\u001b[0m     sparse\u001b[39m.\u001b[39missparse(a)\n\u001b[0;32m    193\u001b[0m     \u001b[39mand\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(b)\n\u001b[0;32m    194\u001b[0m     \u001b[39mand\u001b[39;00m dense_output\n\u001b[0;32m    195\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(ret, \u001b[39m\"\u001b[39m\u001b[39mtoarray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    196\u001b[0m ):\n\u001b[0;32m    197\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39mtoarray()\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\scipy\\sparse\\_base.py:630\u001b[0m, in \u001b[0;36mspmatrix.__matmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m isscalarlike(other):\n\u001b[0;32m    628\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mScalar operands are not allowed, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    629\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39muse \u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m\u001b[39m instead\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 630\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mul_dispatch(other)\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\scipy\\sparse\\_base.py:530\u001b[0m, in \u001b[0;36mspmatrix._mul_dispatch\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mul_vector(other)\n\u001b[0;32m    529\u001b[0m \u001b[39melif\u001b[39;00m other\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (N, \u001b[39m1\u001b[39m):\n\u001b[1;32m--> 530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mul_vector(other\u001b[39m.\u001b[39;49mravel())\u001b[39m.\u001b[39mreshape(M, \u001b[39m1\u001b[39m)\n\u001b[0;32m    531\u001b[0m \u001b[39melif\u001b[39;00m other\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m other\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m N:\n\u001b[0;32m    532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mul_multivector(other)\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\scipy\\sparse\\_dia.py:272\u001b[0m, in \u001b[0;36mdia_matrix._mul_vector\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    268\u001b[0m L \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m    270\u001b[0m M,N \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape\n\u001b[1;32m--> 272\u001b[0m dia_matvec(M,N, \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moffsets), L, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moffsets, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, x\u001b[39m.\u001b[39;49mravel(), y\u001b[39m.\u001b[39;49mravel())\n\u001b[0;32m    274\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set seed of numpy and torch\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Dimension Reduction Hyperparameters\n",
    "ae_params = {\"hidden_dim\": [10],\n",
    "                \"layer_dims\": [[32, 16]],\n",
    "                \"dropout\": [0.1],\n",
    "                \"lr\": [0.001],\n",
    "                \"epochs\": [300]\n",
    "}\n",
    "\n",
    "kpca_params = {\"n_components\": list(np.arange(1, 5, 1)),\n",
    "                \"kernel\": [\"poly\"],\n",
    "                \"degree\": [2, 4],\n",
    "}\n",
    "\n",
    "pca_params = {\"nfac\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 40, 50]}\n",
    "\n",
    "# Regression hyperparameters\n",
    "kkr_params = {\"alpha\": [0.3, 0.5, 0.7],\n",
    "              \"kernel\": [\"rbf\"],\n",
    "              \"gamma\": [0.3, 0.5, 0.7],\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    \"n_estimators\": [200],\n",
    "    \"max_depth\": [5],\n",
    "    \"max_features\": [\"sqrt\"],\n",
    "    \"min_samples_split\": [2],\n",
    "    \"min_samples_leaf\": [1],\n",
    "}\n",
    "\n",
    "# Run the forecasting exercise\n",
    "for target in variables.keys():\n",
    "    if target == 'data':\n",
    "        continue\n",
    "    print(\"Target: \", target)\n",
    "    y = variables[target].values\n",
    "    result = out_sample(\n",
    "        X = X,\n",
    "        y = y,\n",
    "        scale_method=\"regression\",\n",
    "        dim_method=\"pca\",\n",
    "        forecast_method=\"ols\",\n",
    "        hyper_params_grid=pca_params,\n",
    "        h=12,\n",
    "        forecast_params=None,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_ar = result['error_ar']\n",
    "errors_pca = result['error_pca']\n",
    "errors_spca = result['error_spca']\n",
    "\n",
    "#errors = pd.DataFrame({'errors_ar': errors_ar, 'errors_pca': errors_pca.flatten()})\n",
    "#np.set_printoptions(formatter={'all':lambda x: str(x)[:7]})\n",
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})\n",
    "mse_ar = np.mean(errors_ar**2)\n",
    "mse_pca = np.mean(errors_pca**2)\n",
    "mse_spca = np.mean(errors_spca**2)\n",
    "\n",
    "print(\"MSE AR: \", mse_ar)\n",
    "print(\"MSE PCA: \", mse_pca)\n",
    "print(\"MSE SPCA: \", mse_spca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
