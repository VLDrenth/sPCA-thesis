{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from src.helpers.functions import get_data, pc_T, predict_pca, estimate_AR_res, generate_data\n",
    "from src.helpers.functions import select_AR_lag_SIC, winsor, lag_matrix\n",
    "from src.helpers.refactored import ar_forecast, scale_X, reduce_dimensions, loocv_ts, standardize, forecast\n",
    "from src.helpers.autoencoder import Autoencoder\n",
    "from src.helpers.lstm_ae import LSTMAutoencoder\n",
    "from src.helpers.forecast import Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_sample(X, y, dim_method=\"ae\", scale_method=\"distance_correlation\", h=1, hyper_params_grid=None, forecast_method=\"ols\",forecast_params=None, update_period=60, target=\"missing\"):\n",
    "    \"\"\" Function to perform out of sample forecasting \"\"\"\n",
    "    T = y.shape[0]\n",
    "\n",
    "    M = (1984-1959)*12  # In sample periods\n",
    "    N = T - M  # Out of sample periods\n",
    "\n",
    "    forecast_spca = np.zeros((N, 1))  # Forecast errors of scaled PCA\n",
    "    forecast_ar = np.zeros((N, 1))  # Forecast errors of AR model\n",
    "    actual_y = np.zeros((N, 1))  # Actual values of y\n",
    "\n",
    "    # Initialize the models\n",
    "    fc = Forecast(method=forecast_method, hyper_params=forecast_params, h=h)\n",
    "    ae = None\n",
    "\n",
    "    p_max = 3  # Max number of lags for AR(p) model\n",
    "    \n",
    "    # Loop over all out of sample periods\n",
    "    for n in range(N):\n",
    "        # Print every 20 percent\n",
    "        if n % (N // 5) == 0:\n",
    "            print(f\"Out of sample period {n} out of {N} periods\")\n",
    "            \n",
    "        # Use all available data up to time t\n",
    "        X_t = X[:(M + n), :]\n",
    "        y_t = y[:M + n]\n",
    "        actual_y[n] = y[M + n]\n",
    "\n",
    "        # Standardize the data\n",
    "        X_t = standardize(X_t)\n",
    "\n",
    "        # Get number lags\n",
    "        p_AR_star_n = select_AR_lag_SIC(y_t, h, p_max=p_max)\n",
    "\n",
    "        # Compute the forecast of the AR model\n",
    "        forecast_ar[n] = ar_forecast(p_AR_star_n, y_t, h)\n",
    "        \n",
    "        #### STEP 1: Scaling factors ####\n",
    "\n",
    "        # Compute the betas for scaling the variables\n",
    "        beta = scale_X(X_t, y_t, h, method=scale_method, p_AR_star_n=p_AR_star_n)\n",
    "        \n",
    "        # Winsorizing the betas\n",
    "        beta_win = winsor(np.abs(beta), p=(0, 90))\n",
    "\n",
    "        # Scale the factors by the winsorized betas\n",
    "        scaleX_t = X_t * beta_win\n",
    "\n",
    "        #### Intermezzo: Find the optimal number of factors (or other hyperparameters) ####\n",
    "        \n",
    "        if n == 0:\n",
    "            #print(\"Starting hyperparameter optimization\")\n",
    "            hyper_params = loocv_ts(X=X_t, y=y_t, h=h, p_AR_star_n=p_AR_star_n, method=dim_method, scale_method=scale_method, grid=hyper_params_grid)\n",
    "            #print(\"Optimal Dimension Reduction hyperparameters found\")\n",
    "\n",
    "            if dim_method == \"ae\":\n",
    "                # Initialize the autoencoder\n",
    "                ae = Autoencoder(input_dim=X.shape[1], activation=nn.SiLU, hyper_params=hyper_params)\n",
    "\n",
    "                # Train the autoencoder on the in sample data\n",
    "                ae.train_model(scaleX_t, lr=hyper_params.get(\"lr\", 0.001), num_epochs=hyper_params.get(\"epochs\", 300))\n",
    "            elif dim_method == \"lstm\":\n",
    "                # Initialize the autoencoder\n",
    "                ae = LSTMAutoencoder(input_dim=X.shape[1], hyper_params=hyper_params)\n",
    "\n",
    "                # Train the autoencoder on the in sample data\n",
    "                ae.train_model(scaleX_t, lr=hyper_params.get(\"lr\", 0.001), num_epochs=hyper_params.get(\"epochs\", 300))\n",
    "\n",
    "                print(\"Autoencoder training done\")\n",
    "\n",
    "        ### Updating Hyperparameters durign forecasting ###\n",
    "        if n == 0 and n > 0:\n",
    "            if dim_method == \"ae\":\n",
    "                pass\n",
    "            else:\n",
    "                hyper_params = loocv_ts(X=X_t, y=y_t, h=h, p_AR_star_n=p_AR_star_n, method=dim_method, scale_method=scale_method, grid=hyper_params_grid)\n",
    "            \n",
    "        #### STEP 2: Dimension Reduction ####            \n",
    "\n",
    "        # Compute the reduced dimensionality representation of the factors\n",
    "        x_spc = reduce_dimensions(X=scaleX_t, hyper_params=hyper_params, method=dim_method, dim_red_model=ae)\n",
    "\n",
    "        #### STEP 3: Forecasting ####\n",
    "\n",
    "        # Add lag of y_t to the factors\n",
    "        if p_AR_star_n > 0:\n",
    "            x_spc = lag_matrix(x_spc, y_t, p_AR_star_n)\n",
    "            y_t = y_t[p_AR_star_n-1:]\n",
    "                \n",
    "        # Cross validate the hyperparameters once in first period\n",
    "        if n == 0 and forecast_params:\n",
    "            fc.cross_validate(x_spc, y_t, hyper_params=forecast_params)\n",
    "        elif n > 0 and forecast_params:\n",
    "            if n == 0:\n",
    "                fc.cross_validate(x_spc, y_t, hyper_params=forecast_params)\n",
    "\n",
    "        # Compute the forecast of the PCA and scaled PCA model\n",
    "        forecast_spca[n] = fc.predict(x_spc, y_t)\n",
    "\n",
    "    # Compute the forecast errors\n",
    "    error_spca = actual_y - forecast_spca\n",
    "    error_ar = actual_y - forecast_ar\n",
    "    \n",
    "    # Compute the R squared out of sample against the AR model\n",
    "    SSE_spca = np.sum(error_spca**2)\n",
    "    SSE_ar = np.sum(error_ar**2)\n",
    "\n",
    "    R2_spca = (1 - SSE_spca / SSE_ar)\n",
    "\n",
    "    print(\"R2_spca: \", round(R2_spca * 100, 2))\n",
    "\n",
    "    # Save the results to a numpy file for later use\n",
    "    np.save(f\"c:/Users/Vincent/PythonProjects/Thesis/resources/results/forecasts/NOCV_{target}_{dim_method}_{scale_method}_{forecast_method}_h{h}_N{N}_pmax{p_max}_M{M}_R2{round(R2_spca * 100, 2)}.npy\", forecast_spca)\n",
    "    np.save(f\"c:/Users/Vincent/PythonProjects/Thesis/resources/results/errors/NOCV_{target}_{dim_method}_{scale_method}_{forecast_method}_h{h}_N{N}_pmax{p_max}_M{M}_R2{round(R2_spca * 100, 2)}.npy\", error_spca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X:  (720, 123)\n"
     ]
    }
   ],
   "source": [
    "variables = get_data()\n",
    "X = variables['data'].values\n",
    "\n",
    "print(\"Shape of X: \", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:  inflation\n",
      "Out of sample period 0 out of 420 periods\n",
      "Number of model configurations:  21\n",
      "Best model configuration:  {'gamma': 0.001, 'n_components': 1, 'kernel': 'rbf'}\n",
      "Out of sample period 84 out of 420 periods\n",
      "Out of sample period 168 out of 420 periods\n",
      "Out of sample period 252 out of 420 periods\n",
      "Out of sample period 336 out of 420 periods\n",
      "R2_spca:  0.26\n",
      "Target:  unemployment\n",
      "Out of sample period 0 out of 420 periods\n",
      "Number of model configurations:  21\n",
      "Best model configuration:  {'gamma': 1e-06, 'n_components': 5, 'kernel': 'rbf'}\n",
      "Out of sample period 84 out of 420 periods\n",
      "Out of sample period 168 out of 420 periods\n",
      "Out of sample period 252 out of 420 periods\n",
      "Out of sample period 336 out of 420 periods\n",
      "R2_spca:  18.01\n",
      "Target:  ip_growth\n",
      "Out of sample period 0 out of 420 periods\n",
      "Number of model configurations:  21\n",
      "Best model configuration:  {'gamma': 0.001, 'n_components': 5, 'kernel': 'rbf'}\n",
      "Out of sample period 84 out of 420 periods\n",
      "Out of sample period 168 out of 420 periods\n",
      "Out of sample period 252 out of 420 periods\n",
      "Out of sample period 336 out of 420 periods\n",
      "R2_spca:  14.13\n"
     ]
    }
   ],
   "source": [
    "# Set seed of numpy and torch\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Dimension Reduction Hyperparameters\n",
    "ae_params = {\"hidden_dim\": [10],\n",
    "                \"layer_dims\": [[32, 16]],\n",
    "                \"dropout\": [0.1],\n",
    "                \"lr\": [0.001],\n",
    "                \"epochs\": [300]\n",
    "}\n",
    "\n",
    "rbf_params = {\"gamma\": 10**np.arange(-6,-2.5,.5),\n",
    "            \"n_components\": [1, 3, 5],\n",
    "            \"kernel\": [\"rbf\"],\n",
    "}\n",
    "\n",
    "sigmoid_params = {\n",
    "    \"gamma\": 10**np.arange(-6,-1.5,.5),\n",
    "    \"n_components\": [1, 3, 5, 7, 9, 11, 15, 20, 25, 30, 40],\n",
    "    \"kernel\": [\"sigmoid\"],\n",
    "}\n",
    "\n",
    "pca_params = {\"nfac\": [1, 3, 5, 7, 9, 11, 15, 20, 25, 30, 40]}\n",
    "\n",
    "# Regression hyperparameters\n",
    "# TODO: ENTER VAN DIJKS HYPERPARAMETERS\n",
    "krr_params = {\n",
    "              \"kernel\": [\"rbf\"],\n",
    "              \"gamma\": [0.3, 0.5, 0.7],\n",
    "              \"alpha\": [0.3, 0.5, 0.7],\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    \"n_estimators\": [200],\n",
    "    \"max_depth\": [5],\n",
    "    \"max_features\": [\"sqrt\"],\n",
    "    \"min_samples_split\": [2],\n",
    "    \"min_samples_leaf\": [1],\n",
    "}\n",
    "\n",
    "# Run the forecasting exercise\n",
    "for target in variables.keys():\n",
    "    if target == 'data':\n",
    "        continue\n",
    "    \n",
    "    print(\"Target: \", target)\n",
    "    y = variables[target].values\n",
    "    result = out_sample(\n",
    "        X = X,\n",
    "        y = y,\n",
    "        scale_method=\"regression\",\n",
    "        dim_method=\"kpca\",\n",
    "        forecast_method=\"ols\",\n",
    "        hyper_params_grid=rbf_params,\n",
    "        h=1,\n",
    "        forecast_params=None,\n",
    "        target=target,\n",
    "        update_period=1000)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_ar = result['error_ar']\n",
    "errors_pca = result['error_pca']\n",
    "errors_spca = result['error_spca']\n",
    "\n",
    "#errors = pd.DataFrame({'errors_ar': errors_ar, 'errors_pca': errors_pca.flatten()})\n",
    "#np.set_printoptions(formatter={'all':lambda x: str(x)[:7]})\n",
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})\n",
    "mse_ar = np.mean(errors_ar**2)\n",
    "mse_pca = np.mean(errors_pca**2)\n",
    "mse_spca = np.mean(errors_spca**2)\n",
    "\n",
    "print(\"MSE AR: \", mse_ar)\n",
    "print(\"MSE PCA: \", mse_pca)\n",
    "print(\"MSE SPCA: \", mse_spca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
