{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from src.helpers.functions import get_data, pc_T, predict_pca, estimate_AR_res, generate_data\n",
    "from src.helpers.functions import select_AR_lag_SIC, winsor, lag_matrix\n",
    "from src.helpers.ardl_multi import ARDL_multi\n",
    "from src.helpers.regression import linear_reg\n",
    "from src.helpers.refactored import ar_forecast, scale_X, reduce_dims, loocv_ts\n",
    "\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_sample(X, y):\n",
    "    h = 1\n",
    "    T = y.shape[0]\n",
    "\n",
    "    M = (1984-1959)*12  # In sample periods\n",
    "    N = T - M  # Out of sample periods\n",
    "\n",
    "    forecast_pca = np.zeros((N, 1))  # Forecast errors of PCA \n",
    "    forecast_spca = np.zeros((N, 1))  # Forecast errors of scaled PCA\n",
    "    forecast_ar = np.zeros((N, 1))  # Forecast errors of AR model\n",
    "    actual_y = np.zeros((N, 1))  # Actual values of y\n",
    "\n",
    "    # Initialize the regression models\n",
    "    reg_pc = LinearRegression()\n",
    "    reg_spc = LinearRegression()\n",
    "\n",
    "    p_max = 1  # Max number of lags for AR(p) model\n",
    "    nfac = 5\n",
    "    scale_method = \"regression\"\n",
    "\n",
    "    # Loop over all out of sample periods\n",
    "    for n in range(N):\n",
    "        #print(\"Forecast {} out of {}\".format(n, N))\n",
    "\n",
    "        # Use all available data up to time t\n",
    "        X_t = X[:(M + n), :]\n",
    "        y_t = y[:M + n]\n",
    "        actual_y[n] = y[M + n]\n",
    "\n",
    "        # Standardize the data\n",
    "        X_t = (X_t - np.mean(X_t, axis=0)) / np.std(X_t, axis=0)\n",
    "\n",
    "        # Get number lags\n",
    "        p_AR_star_n = select_AR_lag_SIC(y_t, h, p_max=p_max)\n",
    "\n",
    "        # Compute the forecast of the AR model\n",
    "        forecast_ar[n] = ar_forecast(p_AR_star_n, y_t, h)\n",
    "        \n",
    "        #### STEP 1: Scaling factors ####\n",
    "\n",
    "        # Compute the betas for scaling the variables\n",
    "        beta = scale_X(X_t, y_t, h, method=scale_method, p_AR_star_n=p_AR_star_n)\n",
    "        \n",
    "        # Winsorizing the betas\n",
    "        beta_win = winsor(np.abs(beta), p=(0, 90))\n",
    "\n",
    "        # Scale the factors by the winsorized betas\n",
    "        scaleX_t = X_t * beta_win\n",
    "\n",
    "        #### Intermezzo: Find the optimal number of factors ####\n",
    "        \n",
    "        #if n == 0:\n",
    "        #    nfac = loocv_ts(X=X_t, y=y_t, h=h, p_AR_star_n=p_AR_star_n, method=\"pca\")\n",
    "        \n",
    "        #### STEP 2: Dimension Reduction ####\n",
    "        # Compute the principal components\n",
    "        x_pc = reduce_dims(X=X_t, nfac=nfac, method=\"pca\")\n",
    "        x_spc = reduce_dims(X=scaleX_t, nfac=nfac, method=\"kpca\")\n",
    "\n",
    "        #### STEP 3: Forecasting ####\n",
    "\n",
    "        # Add lag of y_t to the factors\n",
    "        if p_AR_star_n > 0:\n",
    "            x_pc = lag_matrix(x_pc, y_t, p_AR_star_n)\n",
    "            x_spc = lag_matrix(x_spc, y_t, p_AR_star_n)\n",
    "            y_t = y_t[p_AR_star_n-1:]\n",
    "            \n",
    "        # Estimate regression coefficients\n",
    "        reg_pc.fit(x_pc[:-h], y_t[h:])\n",
    "        reg_spc.fit(x_spc[:-h], y_t[h:])\n",
    "        \n",
    "        # Compute the forecast of the PCA and scaled PCA model\n",
    "        forecast_pca[n] = reg_pc.predict(x_pc[-1].reshape(1, -1))\n",
    "        forecast_spca[n] = reg_spc.predict(x_spc[-1].reshape(1, -1))\n",
    "\n",
    "    # Compute the forecast errors\n",
    "    error_pca = actual_y - forecast_pca\n",
    "    error_spca = actual_y - forecast_spca\n",
    "    error_ar = actual_y - forecast_ar\n",
    "    \n",
    "    # Compute the R squared out of sample against the AR model\n",
    "    SSE_pca = np.sum(error_pca**2)\n",
    "    SSE_spca = np.sum(error_spca**2)\n",
    "    SSE_ar = np.sum(error_ar**2)\n",
    "\n",
    "    R2_spca = (1 - SSE_spca / SSE_ar)\n",
    "    R2_pca = (1 - SSE_pca / SSE_ar)\n",
    "\n",
    "    print(\"R2_spca: \", round(R2_spca * 100, 2))\n",
    "    print(\"R2_pca: \", round(R2_pca* 100, 2))\n",
    "    \n",
    "    return {\"error_pca\": error_pca,\n",
    "            \"error_spca\": error_spca,\n",
    "            \"error_ar\": error_ar,\n",
    "            \"R2_spca\": R2_spca,\n",
    "            \"R2_pca\": R2_pca}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = get_data()\n",
    "X = variables['data'].values\n",
    "y = variables['unemployment'].values\n",
    "\n",
    "print(\"Shape of X: \", X.shape, \"Shape of y: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:  inflation\n",
      "R2_spca:  6.34\n",
      "R2_pca:  -14.11\n",
      "Target:  unemployment\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[537], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTarget: \u001b[39m\u001b[39m\"\u001b[39m, target)\n\u001b[0;32m      6\u001b[0m y \u001b[39m=\u001b[39m variables[target]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m----> 7\u001b[0m result \u001b[39m=\u001b[39m out_sample(X \u001b[39m=\u001b[39;49m X, y \u001b[39m=\u001b[39;49m y)\n",
      "Cell \u001b[1;32mIn[536], line 71\u001b[0m, in \u001b[0;36mout_sample\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39m# Estimate regression coefficients\u001b[39;00m\n\u001b[0;32m     70\u001b[0m reg_pc\u001b[39m.\u001b[39mfit(x_pc[:\u001b[39m-\u001b[39mh], y_t[h:])\n\u001b[1;32m---> 71\u001b[0m reg_spc\u001b[39m.\u001b[39;49mfit(x_spc[:\u001b[39m-\u001b[39;49mh], y_t[h:])\n\u001b[0;32m     73\u001b[0m \u001b[39m# Compute the forecast of the PCA and scaled PCA model\u001b[39;00m\n\u001b[0;32m     74\u001b[0m forecast_pca[n] \u001b[39m=\u001b[39m reg_pc\u001b[39m.\u001b[39mpredict(x_pc[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    462\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    464\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    465\u001b[0m ]\n\u001b[0;32m    467\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    474\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    475\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    476\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    477\u001b[0m )(\n\u001b[0;32m    478\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    479\u001b[0m         t,\n\u001b[0;32m    480\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    481\u001b[0m         X,\n\u001b[0;32m    482\u001b[0m         y,\n\u001b[0;32m    483\u001b[0m         sample_weight,\n\u001b[0;32m    484\u001b[0m         i,\n\u001b[0;32m    485\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    486\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    487\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    488\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    489\u001b[0m     )\n\u001b[0;32m    490\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    493\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    182\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 184\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\sklearn\\tree\\_classes.py:1247\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1219\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m \n\u001b[0;32m   1221\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1247\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m   1248\u001b[0m         X,\n\u001b[0;32m   1249\u001b[0m         y,\n\u001b[0;32m   1250\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1251\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m   1252\u001b[0m     )\n\u001b[0;32m   1253\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the forecasting exercise\n",
    "for target in variables.keys():\n",
    "    if target == 'data':\n",
    "        continue\n",
    "    print(\"Target: \", target)\n",
    "    y = variables[target].values\n",
    "    result = out_sample(X = X, y = y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression scaling, kpca RBF\n",
    "Target:  inflation \n",
    "R2_spca:  12.39\n",
    "R2_pca:  -0.76\n",
    "Target:  unemployment\n",
    "R2_spca:  17.22\n",
    "R2_pca:  10.12\n",
    "Target:  ip_growth\n",
    "R2_spca:  13.22\n",
    "R2_pca:  7.88\n",
    "Target:  volatility\n",
    "R2_spca:  5.4\n",
    "R2_pca:  1.13\n",
    "\n",
    "#### Distance correlation scaling, kpca RBF\n",
    "Target:  inflation\n",
    "R2_spca:  9.03\n",
    "R2_pca:  -0.76\n",
    "Target:  unemployment\n",
    "R2_spca:  17.59\n",
    "R2_pca:  10.12\n",
    "Target:  ip_growth\n",
    "R2_spca:  10.88\n",
    "R2_pca:  7.88\n",
    "Target:  volatility\n",
    "R2_spca:  5.66\n",
    "R2_pca:  1.13\n",
    "\n",
    "#### Distance correlation scaling, kpca poly(5)\n",
    "Target:  inflation\n",
    "R2_spca:  9.73\n",
    "R2_pca:  -0.76\n",
    "Target:  unemployment\n",
    "R2_spca:  18.86\n",
    "R2_pca:  10.12\n",
    "Target:  ip_growth\n",
    "R2_spca:  12.63\n",
    "R2_pca:  7.88\n",
    "Target:  volatility\n",
    "R2_spca:  4.63\n",
    "R2_pca:  1.13\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_ar = result['error_ar']\n",
    "errors_pca = result['error_pca']\n",
    "errors_spca = result['error_spca']\n",
    "\n",
    "#errors = pd.DataFrame({'errors_ar': errors_ar, 'errors_pca': errors_pca.flatten()})\n",
    "#np.set_printoptions(formatter={'all':lambda x: str(x)[:7]})\n",
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})\n",
    "mse_ar = np.mean(errors_ar**2)\n",
    "mse_pca = np.mean(errors_pca**2)\n",
    "mse_spca = np.mean(errors_spca**2)\n",
    "\n",
    "print(\"MSE AR: \", mse_ar)\n",
    "print(\"MSE PCA: \", mse_pca)\n",
    "print(\"MSE SPCA: \", mse_spca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
