{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from src.helpers.functions import get_data, pc_T, predict_pca, estimate_AR_res, generate_data\n",
    "from src.helpers.functions import select_AR_lag_SIC, winsor, lag_matrix\n",
    "from src.helpers.refactored import ar_forecast, scale_X, reduce_dimensions, loocv_ts, standardize, forecast\n",
    "from src.helpers.autoencoder import Autoencoder\n",
    "from src.helpers.lstm_ae import LSTMAutoencoder\n",
    "from src.helpers.forecast import Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_sample(X, y, dim_method=\"ae\", scale_method=\"distance_correlation\", h=1, hyper_params_grid=None, forecast_method=\"ols\",forecast_params=None, update_period=60, target=\"missing\"):\n",
    "    \"\"\" Function to perform out of sample forecasting \"\"\"\n",
    "    T = y.shape[0]\n",
    "\n",
    "    M = (2000-1959)*12  # In sample periods\n",
    "    N = T - M  # Out of sample periods\n",
    "\n",
    "    forecast_spca = np.zeros((N, 1))  # Forecast errors of scaled PCA\n",
    "    forecast_ar = np.zeros((N, 1))  # Forecast errors of AR model\n",
    "    actual_y = np.zeros((N, 1))  # Actual values of y\n",
    "\n",
    "    # Initialize the models\n",
    "    fc = Forecast(method=forecast_method, hyper_params=forecast_params, h=h)\n",
    "    ae = None\n",
    "\n",
    "    p_max = 3  # Max number of lags for AR(p) model\n",
    "    \n",
    "    # Loop over all out of sample periods\n",
    "    for n in range(N):\n",
    "        # Print every 20 percent\n",
    "        if n % (N // 5) == 0:\n",
    "            print(f\"Out of sample period {n} out of {N} periods\")\n",
    "            \n",
    "        # Use all available data up to time t\n",
    "        X_t = X[:(M + n), :]\n",
    "        y_t = y[:M + n]\n",
    "        actual_y[n] = y[M + n]\n",
    "\n",
    "        # Standardize the data\n",
    "        X_t = standardize(X_t)\n",
    "\n",
    "        # Get number lags\n",
    "        p_AR_star_n = select_AR_lag_SIC(y_t, h, p_max=p_max)\n",
    "\n",
    "        # Compute the forecast of the AR model\n",
    "        forecast_ar[n] = ar_forecast(p_AR_star_n, y_t, h)\n",
    "        \n",
    "        #### STEP 1: Scaling factors ####\n",
    "\n",
    "        # Compute the betas for scaling the variables\n",
    "        beta = scale_X(X_t, y_t, h, method=scale_method, p_AR_star_n=p_AR_star_n)\n",
    "        \n",
    "        # Winsorizing the betas\n",
    "        beta_win = winsor(np.abs(beta), p=(0, 90))\n",
    "\n",
    "        # Scale the factors by the winsorized betas\n",
    "        scaleX_t = X_t * beta_win\n",
    "\n",
    "        #### Intermezzo: Find the optimal number of factors (or other hyperparameters) ####\n",
    "        \n",
    "        if n == 0:\n",
    "            #print(\"Starting hyperparameter optimization\")\n",
    "            hyper_params = loocv_ts(X=scaleX_t, y=y_t, h=h, p_AR_star_n=p_AR_star_n, method=dim_method, scale_method=scale_method, grid=hyper_params_grid)\n",
    "            #print(\"Optimal Dimension Reduction hyperparameters found\")\n",
    "            print(\"-----------------------------------------------------------------\")\n",
    "            print(\"Initial hyperparameter optimization done\")\n",
    "            \n",
    "            if dim_method == \"ae\":\n",
    "                # Initialize the autoencoder\n",
    "                ae = Autoencoder(input_dim=X.shape[1], activation=nn.SiLU, hyper_params=hyper_params)\n",
    "\n",
    "                # Train the autoencoder on the in sample data\n",
    "                ae.train_model(scaleX_t, lr=hyper_params.get(\"lr\", 0.001), num_epochs=hyper_params.get(\"epochs\", 300))\n",
    "            elif dim_method == \"lstm\":\n",
    "                # Initialize the autoencoder\n",
    "                ae = LSTMAutoencoder(input_dim=X.shape[1], hyper_params=hyper_params)\n",
    "\n",
    "                # Train the autoencoder on the in sample data\n",
    "                ae.train_model(scaleX_t, lr=hyper_params.get(\"lr\", 0.001), num_epochs=hyper_params.get(\"epochs\", 300))\n",
    "\n",
    "                print(\"Autoencoder training done\")\n",
    "\n",
    "        ### Updating Hyperparameters durign forecasting ###\n",
    "        if n % update_period == 0 and n > 0:\n",
    "            if dim_method == \"ae\":\n",
    "                pass\n",
    "            else:\n",
    "                hyper_params = loocv_ts(X=X_t, y=y_t, h=h, p_AR_star_n=p_AR_star_n, method=dim_method, scale_method=scale_method, grid=hyper_params_grid)\n",
    "            \n",
    "        #### STEP 2: Dimension Reduction ####            \n",
    "\n",
    "        # Compute the reduced dimensionality representation of the factors\n",
    "        x_spc = reduce_dimensions(X=scaleX_t, hyper_params=hyper_params, method=dim_method, dim_red_model=ae)\n",
    "\n",
    "        #### STEP 3: Forecasting ####\n",
    "\n",
    "        # Add lag of y_t to the factors\n",
    "        if p_AR_star_n > 0:\n",
    "            x_spc = lag_matrix(x_spc, y_t, p_AR_star_n)\n",
    "            y_t = y_t[p_AR_star_n-1:]\n",
    "                \n",
    "        # Cross validate the hyperparameters once in first period\n",
    "        if n == 0 and forecast_params:\n",
    "            fc.cross_validate(x_spc, y_t, hyper_params=forecast_params)\n",
    "        elif n > 0 and forecast_params:\n",
    "            if n % update_period == 0:\n",
    "                fc.cross_validate(x_spc, y_t, hyper_params=forecast_params)\n",
    "\n",
    "        # Compute the forecast of the PCA and scaled PCA model\n",
    "        forecast_spca[n] = fc.predict(x_spc, y_t)\n",
    "\n",
    "    # Compute the forecast errors\n",
    "    error_spca = actual_y - forecast_spca\n",
    "    error_ar = actual_y - forecast_ar\n",
    "    \n",
    "    # Compute the R squared out of sample against the AR model\n",
    "    SSE_spca = np.sum(error_spca**2)\n",
    "    SSE_ar = np.sum(error_ar**2)\n",
    "\n",
    "    R2_spca = (1 - SSE_spca / SSE_ar)\n",
    "\n",
    "    print(\"R2_spca: \", round(R2_spca * 100, 2))\n",
    "\n",
    "    if dim_method == \"kpca\":\n",
    "        dim_method += \"_\" + hyper_params['kernel']\n",
    "        \n",
    "    # Save the results to a numpy file for later use\n",
    "    #np.save(f\"c:/Users/Vincent/PythonProjects/Thesis/resources/results/forecasts/NOCV_{target}_{dim_method}_{scale_method}_{forecast_method}_h{h}_N{N}_pmax{p_max}_M{M}_R2{round(R2_spca * 100, 2)}.npy\", forecast_spca)\n",
    "    #np.save(f\"c:/Users/Vincent/PythonProjects/Thesis/resources/results/errors/NOCV_{target}_{dim_method}_{scale_method}_{forecast_method}_h{h}_N{N}_pmax{p_max}_M{M}_R2{round(R2_spca * 100, 2)}.npy\", error_spca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X:  (720, 123)\n"
     ]
    }
   ],
   "source": [
    "variables = get_data()\n",
    "X = variables['data'].values\n",
    "\n",
    "print(\"Shape of X: \", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [12:54<00:00, 31.00s/trial, best loss: 8.881653785705566]\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "# define the space of hyperparameters to search\n",
    "space = {\n",
    "    'hidden_dim': hp.choice('hidden_dim', list(np.arange(1, 30, 2))),\n",
    "    'layer_dims': hp.choice('layer_dims', [[64, 32], [32, 32], [100, 50], [64, 64], [32, 32, 32], [120, 80, 40]]),\n",
    "    'activation': hp.choice('activation', [nn.SiLU, nn.LeakyReLU]),\n",
    "    'gauss_noise': hp.uniform('gauss_noise', 0, 1),\n",
    "    'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "    'lr': hp.loguniform('lr', -5, -2),\n",
    "    'num_epochs': hp.choice('num_epochs', [200, 300, 400]),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 100])\n",
    "}\n",
    "\n",
    "def evaluate_model(model, X_valid):\n",
    "    model.eval()  # set the model to evaluation mode\n",
    "\n",
    "    # Convert to torch tensor\n",
    "    X_valid = torch.from_numpy(X_valid).float().to(model.device)\n",
    "\n",
    "    # Reshape and run through model\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], 1, X_valid.shape[1])\n",
    "    output = model(X_valid)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = model.loss_criterion(output, X_valid)\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def objective(params):\n",
    "    # load your data\n",
    "    variables = get_data()\n",
    "    X = variables['data'].values\n",
    "\n",
    "    X_train, X_valid = X[:int(0.8*len(X))], X[int(0.8*len(X)):]\n",
    "\n",
    "    # create an instance of the model\n",
    "    model = Autoencoder(hyper_params=params)\n",
    "    \n",
    "    # train the model\n",
    "    model.train_model(X_train, lr=params['lr'], num_epochs=params['num_epochs'], batch_size=params['batch_size'])\n",
    "\n",
    "    # evaluate the model\n",
    "    val_loss = evaluate_model(model, X_valid)  # you need to implement this function\n",
    "\n",
    "    return {'loss': val_loss, 'status': STATUS_OK}\n",
    "\n",
    "# run the optimizer\n",
    "trials = Trials()\n",
    "best = fmin(objective, space, algo=tpe.suggest, max_evals=25, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 0, 'batch_size': 2, 'dropout': 0.44937695325960963, 'gauss_noise': 0.7412079106731622, 'hidden_dim': 0, 'layer_dims': 0, 'lr': 0.02766271599852037, 'num_epochs': 1}\n"
     ]
    }
   ],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:  inflation\n",
      "Out of sample period 0 out of 228 periods\n",
      "Number of model configurations:  10\n",
      "Best model configuration:  {'nfac': 11}\n",
      "-----------------------------------------------------------------\n",
      "Initial hyperparameter optimization done\n",
      "Out of sample period 45 out of 228 periods\n",
      "Out of sample period 90 out of 228 periods\n",
      "Out of sample period 135 out of 228 periods\n",
      "Out of sample period 180 out of 228 periods\n",
      "Out of sample period 225 out of 228 periods\n",
      "R2_spca:  -16.28\n"
     ]
    }
   ],
   "source": [
    "# Set seed of numpy and torch\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Dimension Reduction Hyperparameters\n",
    "ae_params = {\"hidden_dim\": [ 10, 15, 20],\n",
    "                \"layer_dims\": [[32], [64]],\n",
    "                \"dropout\": [0.45],\n",
    "                \"gauss_noise\": [0.2, 0.5],\n",
    "                \"lr\": [0.03],\n",
    "                \"epochs\": [200],\n",
    "                \"update_epochs\": [15],\n",
    "                \"update_lr\": [0.001]\n",
    "}\n",
    "\n",
    "rbf_params = {\"gamma\": 10**np.arange(-6,-2.5,.5),\n",
    "            \"n_components\": [1, 3, 5, 7, 9, 11, 15, 20, 25, 30],\n",
    "            \"kernel\": [\"rbf\"],\n",
    "}\n",
    "\n",
    "sigmoid_params = {\n",
    "    \"gamma\": 10**np.arange(-6,-1.5,.5),\n",
    "    \"n_components\": [1, 3, 5, 7, 9, 11, 15, 20, 25, 30],\n",
    "    \"kernel\": [\"sigmoid\"],\n",
    "}\n",
    "\n",
    "pca_params = {\"nfac\": [1, 3, 5, 7, 9, 11, 15, 20, 25, 30]}\n",
    "\n",
    "# Regression hyperparameters\n",
    "# TODO: ENTER EXTERTATE HYPERPARAMETERS\n",
    "krr_params = {\n",
    "              \"kernel\": [\"rbf\"],\n",
    "              \"gamma\": [0.3, 0.5, 0.7],\n",
    "              \"alpha\": [0.3, 0.5, 0.7],\n",
    "}\n",
    "\n",
    "rf_params_grid = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [5, None],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 3],\n",
    "}\n",
    "\n",
    "# Run the forecasting exercise\n",
    "for target in variables.keys():\n",
    "    if target != 'inflation':\n",
    "        continue\n",
    "    \n",
    "    print(\"Target: \", target)\n",
    "    y = variables[target].values\n",
    "    result = out_sample(\n",
    "        X = X,\n",
    "        y = y,\n",
    "        scale_method=\"regression\",\n",
    "        dim_method=\"pca\",\n",
    "        forecast_method=\"ols\",\n",
    "        hyper_params_grid=pca_params,\n",
    "        h=3,\n",
    "        forecast_params=None,\n",
    "        target=target,\n",
    "        update_period=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_ar = result['error_ar']\n",
    "errors_pca = result['error_pca']\n",
    "errors_spca = result['error_spca']\n",
    "\n",
    "#errors = pd.DataFrame({'errors_ar': errors_ar, 'errors_pca': errors_pca.flatten()})\n",
    "#np.set_printoptions(formatter={'all':lambda x: str(x)[:7]})\n",
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})\n",
    "mse_ar = np.mean(errors_ar**2)\n",
    "mse_pca = np.mean(errors_pca**2)\n",
    "mse_spca = np.mean(errors_spca**2)\n",
    "\n",
    "print(\"MSE AR: \", mse_ar)\n",
    "print(\"MSE PCA: \", mse_pca)\n",
    "print(\"MSE SPCA: \", mse_spca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
