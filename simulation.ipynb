{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "from src.helpers.functions import winsor, corr_uniform, sPCAest\n",
    "from scipy.ndimage import shift\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA, KernelPCA, SparsePCA, FastICA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weak factor case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250,)\n",
      "(250,)\n",
      "(250, 500)\n",
      "y_t:  [ 2.04913679 -1.64999425 -1.30016159  0.83831353 -4.52415919  1.20956829\n",
      " -0.8243528   0.77193026 -2.38909931  0.33492328]\n",
      "y_h:  [-1.64999425 -1.30016159  0.83831353 -4.52415919  1.20956829 -0.8243528\n",
      "  0.77193026 -2.38909931  0.33492328  1.20228037]\n",
      "\n",
      "e:  [-1.1674389  -0.50939583 -0.02538958 -1.4283239  -0.57223265 -1.14476396\n",
      "  0.52698544 -1.13888465 -0.46207751  1.87028361]\n",
      "g:  [-0.48255535 -0.79076575  0.86370311 -3.09583529  1.78180094  0.32041116\n",
      "  0.24494482 -1.25021467  0.79700079 -0.66800324]\n"
     ]
    }
   ],
   "source": [
    "def generate_data(n, T, N, h_steps=1, heteroskedastic=False, psi_max=None, rho=None):\n",
    "    # Check if weak or strong factor model\n",
    "    if n == N:\n",
    "        # Strong factor model, require psi_max and rho\n",
    "        assert psi_max is not None\n",
    "        assert rho is not None\n",
    "\n",
    "    # Generate y_{t + h} = g_t + e_{t + h}\n",
    "    # e_{t + h} ~ N(0, 1)\n",
    "    # g_t ~ N(0, 1)\n",
    "    # X_t,i = g_t*phi_i + h_t*psi_i + u_t,i\n",
    "\n",
    "    # Set T to T + h to account for lag\n",
    "    T_plus_h = T + h_steps\n",
    "\n",
    "    # Generate g_t\n",
    "    g = np.zeros(T_plus_h)\n",
    "    g = np.random.normal(0, 1, T_plus_h)\n",
    "\n",
    "    # Generate h_t\n",
    "    h = np.zeros(T_plus_h)\n",
    "    h = np.random.normal(0, 1, T_plus_h)\n",
    "    \n",
    "    # Generate e_t+h\n",
    "    e = np.zeros(T_plus_h)\n",
    "    e = np.random.normal(0, 1, T_plus_h)\n",
    "\n",
    "    # Generate y_{t+h}\n",
    "    y_h = np.zeros(T_plus_h)\n",
    "    y_h = g + e\n",
    "\n",
    "    # Generate y_t\n",
    "    y_t = np.zeros(T_plus_h)\n",
    "    y_t[h_steps:T_plus_h] = y_h[:T]\n",
    "\n",
    "    phi = np.zeros(N)\n",
    "    psi = np.zeros(N)\n",
    "\n",
    "    if n < N:\n",
    "        # Generate phi_i (Nx1) with n < N nonzero elements\n",
    "        phi[:n] = np.random.uniform(0, 1, n)\n",
    "\n",
    "        # Generate psi_i (Nx1) with n < N nonzero elements\n",
    "        psi[:n] = np.random.uniform(0, 1, n)\n",
    "\n",
    "        # Generate idiosyncratic error's variances\n",
    "        variance_u = np.random.uniform(0, 1, N)        \n",
    "    elif n == N:\n",
    "        # Phi is U(0, phi_max)\n",
    "        psi = np.random.uniform(0, psi_max, N)\n",
    "\n",
    "        # Draw correlated phi and sigma_u\n",
    "        # Generate correlation matrix for phi and sigma_u\n",
    "        phi, variance_u = corr_uniform(rho=rho*1.1, size=N)\n",
    "\n",
    "    u = np.zeros((T, N))\n",
    "    # Generate u_t,i depending on whether the model is heteroskedastic or not\n",
    "    if heteroskedastic:\n",
    "        for t in range(T):\n",
    "            scale = np.random.uniform(0.5, 1.5)\n",
    "            u[t, :] = np.random.normal(0, variance_u * scale, N)\n",
    "    else:\n",
    "        # Generate u\n",
    "        u = np.random.multivariate_normal(mean=np.zeros(N), cov=np.diag(variance_u), size=T)\n",
    "\n",
    "    # Drop first h rows\n",
    "    y_h = y_h[h_steps:]\n",
    "    y_t = y_t[h_steps:]\n",
    "    e = e[h_steps:]\n",
    "    g = g[h_steps:]\n",
    "    h = h[h_steps:]\n",
    "\n",
    "    X = g[:, np.newaxis]*phi[np.newaxis, :] + h[:, np.newaxis]*psi[np.newaxis, :] + u\n",
    "\n",
    "    results = {'y_t':y_t,\n",
    "               'y_h':y_h,\n",
    "                'X':X,\n",
    "                'g':g,\n",
    "                'e':e,\n",
    "                }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Testing\n",
    "results = generate_data(10, T=250, N=500, h_steps=1)\n",
    "y_t = results['y_t']\n",
    "y_h = results['y_h']\n",
    "X = results['X']\n",
    "e = results['e']\n",
    "g = results['g']\n",
    "\n",
    "print(y_t.shape)\n",
    "print(y_h.shape)\n",
    "print(X.shape)\n",
    "\n",
    "print('y_t: ', y_t[:10])\n",
    "print('y_h: ', y_h[:10])\n",
    "print()\n",
    "print('e: ', e[:10])\n",
    "print('g: ', g[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pca(X, y_h, y_t, h_steps=1, nfac=2, method=\"sPCA\", start_test=200):\n",
    "    pca = PCA(n_components=nfac)\n",
    "    reg = LinearRegression()\n",
    "\n",
    "    predicted = np.zeros((1, 50 - h_steps - 1))\n",
    "    error_mat = np.zeros((1, 50 - h_steps - 1))\n",
    "    normalize = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "    for t in range(50 - h_steps - 1):\n",
    "        # Split data into training and testing sets\n",
    "        # Training set is the first 200 + t observations\n",
    "        # Testing set is the last 50 - t observations\n",
    "        idx_split = start_test + t\n",
    "        \n",
    "        X_train = X[:idx_split]\n",
    "        y_train = y_t[:idx_split]\n",
    "        y_test = y_t[idx_split]\n",
    "\n",
    "        if method == \"sPCA\":\n",
    "            # Estimate the parameters of the model using sPCAest\n",
    "            # The function sPCAest is defined in functions.py\n",
    "            factors, _ = sPCAest(y_train, X_train, nfac,[0, 100], h_steps)\n",
    "        elif method == \"PCA\":\n",
    "            X_normalized = normalize.fit_transform(X_train)\n",
    "            factors = pca.fit_transform(X_normalized)\n",
    "\n",
    "        reg.fit(factors[:-h_steps,:], y_train[h_steps:])\n",
    "        \n",
    "        # Predict y_{t+h} using the estimated parameters\n",
    "        y_pred = reg.predict(factors[-1].reshape(1, -1))\n",
    "\n",
    "        # Add predicted value to the predicted vector\n",
    "        predicted[0, t] = y_pred\n",
    "        \n",
    "        # Compute the error\n",
    "        error_mat[0, t] = y_pred - y_test\n",
    "\n",
    "    return error_mat, predicted\n",
    "\n",
    "\n",
    "def simulate_PCA(n=10, T=250, N=500, h_steps=1, R=10, nfac=2, method=\"sPCA\", heteroskedastic=False, psi_max=None, rho=None):\n",
    "    # Initialize the MSE vector\n",
    "    predicted = np.zeros((R, 50 - h_steps - 1))\n",
    "    error_mat = np.zeros((R, 50 - h_steps - 1))\n",
    "\n",
    "    start_test = 200\n",
    "\n",
    "    for r in tqdm(range(R)):\n",
    "        variables = generate_data(n=n, T=T, h_steps=h_steps, N=N, heteroskedastic=heteroskedastic, psi_max=psi_max, rho=rho)\n",
    "\n",
    "        # y_t contains the values of y at time t, and y_h contains the values of y at time t+h\n",
    "        # X contains the values of X at time t\n",
    "        y_h = variables['y_h']\n",
    "        y_t = variables['y_t']\n",
    "        X = variables['X']\n",
    "        \n",
    "        # Loop for expanding window estimation\n",
    "        # The initial window size is 200 observations\n",
    "        # The window size is increased by 1 observation at each iteration\n",
    "        # The window size is increased until it reaches 250 observations\n",
    "        errors, predictions = predict_pca(X, y_h, y_t, h_steps, nfac, method, start_test)\n",
    "        error_mat[r, :] = errors\n",
    "        predicted[r, :] = predictions\n",
    "        \n",
    "    return error_mat, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nfac: 1, method: sPCA, heterosked: True, n: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:53<00:00, 17.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.19301935 1.91638123 1.60194477 1.40395473 1.03002431 1.88759607\n",
      " 0.89039235 1.29198268 1.59367375 1.51065004]\n",
      "Median MSE: 1.4573023865088728\n",
      "nfac: 1, method: sPCA, heterosked: True, n: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:39<02:39, 19.99s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m [\u001b[39m10\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m30\u001b[39m, \u001b[39m40\u001b[39m, \u001b[39m50\u001b[39m]:\n\u001b[0;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnfac: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, method: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, heterosked: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, n: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(nfac, method, heterosked, n))\n\u001b[1;32m---> 11\u001b[0m     errors, predictions \u001b[39m=\u001b[39m simulate_PCA(n\u001b[39m=\u001b[39;49mn, T\u001b[39m=\u001b[39;49m\u001b[39m250\u001b[39;49m, N\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m, h_steps\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, R\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, nfac\u001b[39m=\u001b[39;49mnfac, method\u001b[39m=\u001b[39;49mmethod, heteroskedastic\u001b[39m=\u001b[39;49mheterosked)\n\u001b[0;32m     13\u001b[0m     \u001b[39m# Save errors in npy file\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[39m#np.save(\"resources/results/sim/newerrors_nfac_{}_method_{}_heterosked_{}_n_{}\".format(nfac, method, heterosked, n), errors)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[39m#print median mse\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     mse_vec \u001b[39m=\u001b[39m (errors\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 61\u001b[0m, in \u001b[0;36msimulate_PCA\u001b[1;34m(n, T, N, h_steps, R, nfac, method, heteroskedastic, psi_max, rho)\u001b[0m\n\u001b[0;32m     55\u001b[0m X \u001b[39m=\u001b[39m variables[\u001b[39m'\u001b[39m\u001b[39mX\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     57\u001b[0m \u001b[39m# Loop for expanding window estimation\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39m# The initial window size is 200 observations\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39m# The window size is increased by 1 observation at each iteration\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m# The window size is increased until it reaches 250 observations\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m errors, predictions \u001b[39m=\u001b[39m predict_pca(X, y_h, y_t, h_steps, nfac, method, start_test)\n\u001b[0;32m     62\u001b[0m error_mat[r, :] \u001b[39m=\u001b[39m errors\n\u001b[0;32m     63\u001b[0m predicted[r, :] \u001b[39m=\u001b[39m predictions\n",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m, in \u001b[0;36mpredict_pca\u001b[1;34m(X, y_h, y_t, h_steps, nfac, method, start_test)\u001b[0m\n\u001b[0;32m     17\u001b[0m y_test \u001b[39m=\u001b[39m y_t[idx_split]\n\u001b[0;32m     19\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msPCA\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     20\u001b[0m     \u001b[39m# Estimate the parameters of the model using sPCAest\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[39m# The function sPCAest is defined in functions.py\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     factors, _ \u001b[39m=\u001b[39m sPCAest(y_train, X_train, nfac,[\u001b[39m0\u001b[39;49m, \u001b[39m100\u001b[39;49m], h_steps)\n\u001b[0;32m     23\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPCA\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     24\u001b[0m     X_normalized \u001b[39m=\u001b[39m normalize\u001b[39m.\u001b[39mfit_transform(X_train)\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\PythonProjects\\Thesis\\src\\helpers\\functions.py:76\u001b[0m, in \u001b[0;36msPCAest\u001b[1;34m(target, X, nfac, quantile, h_steps)\u001b[0m\n\u001b[0;32m     73\u001b[0m lr \u001b[39m=\u001b[39m LinearRegression(fit_intercept\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     75\u001b[0m \u001b[39m# Drop the last h_steps observations from Xs and target to avoid look-ahead bias\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m lr\u001b[39m.\u001b[39;49mfit(Xs[:\u001b[39m-\u001b[39;49mh_steps, j]\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m), target[h_steps:])\n\u001b[0;32m     77\u001b[0m beta[j] \u001b[39m=\u001b[39m lr\u001b[39m.\u001b[39mcoef_[\u001b[39m0\u001b[39m]\n\u001b[0;32m     79\u001b[0m \u001b[39m#xvar = np.column_stack((np.ones(T), Xs[:, j]))\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[39m#parm = np.linalg.lstsq(xvar, target, rcond=None)[0]\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[39m#beta[0, j] = parm[1]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_base.py:648\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    644\u001b[0m n_jobs_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs\n\u001b[0;32m    646\u001b[0m accept_sparse \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositive \u001b[39melse\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcoo\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> 648\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    649\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39;49maccept_sparse, y_numeric\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m    650\u001b[0m )\n\u001b[0;32m    652\u001b[0m sample_weight \u001b[39m=\u001b[39m _check_sample_weight(\n\u001b[0;32m    653\u001b[0m     sample_weight, X, dtype\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdtype, only_non_negative\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    654\u001b[0m )\n\u001b[0;32m    656\u001b[0m X, y, X_offset, y_offset, X_scale \u001b[39m=\u001b[39m _preprocess_data(\n\u001b[0;32m    657\u001b[0m     X,\n\u001b[0;32m    658\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    661\u001b[0m     sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[0;32m    662\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\sklearn\\base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    583\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    585\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\sklearn\\utils\\validation.py:1124\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m-> 1124\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1126\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\sklearn\\utils\\validation.py:395\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check that all arrays have consistent first dimensions.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[39mChecks whether all objects in arrays have the same shape or length.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[39m    Objects that will be checked for consistent length.\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    394\u001b[0m lengths \u001b[39m=\u001b[39m [_num_samples(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m arrays \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m--> 395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[0;32m    272\u001b[0m ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(ar)\n\u001b[0;32m    273\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     ret \u001b[39m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[0;32m    275\u001b[0m                     equal_nan\u001b[39m=\u001b[39;49mequal_nan)\n\u001b[0;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[0;32m    278\u001b[0m \u001b[39m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\arraysetops.py:328\u001b[0m, in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_unique1d\u001b[39m(ar, return_index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, return_inverse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    324\u001b[0m               return_counts\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m, equal_nan\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    325\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[39m    Find the unique elements of an array, ignoring shape.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 328\u001b[0m     ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masanyarray(ar)\u001b[39m.\u001b[39;49mflatten()\n\u001b[0;32m    330\u001b[0m     optional_indices \u001b[39m=\u001b[39m return_index \u001b[39mor\u001b[39;00m return_inverse\n\u001b[0;32m    332\u001b[0m     \u001b[39mif\u001b[39;00m optional_indices:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Run the simulationc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:729\n",
    "\n",
    "for nfac in [1, 2, 3]:\n",
    "    for method in [\"sPCA\"]:\n",
    "        for heterosked in [True, False]:\n",
    "            for n in [10, 20, 30, 40, 50]:\n",
    "                print(\"nfac: {}, method: {}, heterosked: {}, n: {}\".format(nfac, method, heterosked, n))\n",
    "                errors, predictions = simulate_PCA(n=n, T=250, N=500, h_steps=1, R=10, nfac=nfac, method=method, heteroskedastic=heterosked)\n",
    "\n",
    "                # Save errors in npy file\n",
    "                #np.save(\"resources/results/sim/newerrors_nfac_{}_method_{}_heterosked_{}_n_{}\".format(nfac, method, heterosked, n), errors)\n",
    "                #print median mse\n",
    "                mse_vec = (errors**2).mean(axis=1)\n",
    "                print(mse_vec)\n",
    "                print(\"Median MSE: {}\".format(np.median(mse_vec)))\n",
    "\n",
    "#errors, predictions = simulate_PCA(n=50, T=250, N=500, h_steps=1, R=100, nfac=2, method=\"PCA\", heteroskedastic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median MSE is 1.2032784403170285\n",
      "Mean MSE is 1.3454188785284196\n",
      "MSE is [1.1533483  1.67962989 1.20327844]\n",
      "[[-1.37698031e+00 -1.51825559e+00 -2.45048694e+00 -7.93658656e-02\n",
      "   5.95462830e-01 -2.33476740e+00 -7.59228655e-01 -4.68956948e-01\n",
      "   9.11057507e-01  5.46488803e-01  1.80725337e-01  1.29629386e+00\n",
      "   2.11389521e-01  3.95513080e-01  2.95997324e-01 -1.49515763e+00\n",
      "   1.76824381e+00  1.69391660e+00  1.10807763e+00  1.11321922e+00\n",
      "   6.67677012e-02 -6.46077988e-01  8.01870294e-02  5.23966656e-01\n",
      "   1.35655611e+00  6.63764057e-01 -5.32275170e-01  9.97696500e-02\n",
      "  -5.45598588e-01  1.41257072e+00  2.94573106e-02 -3.51512564e-01\n",
      "  -1.18330142e+00 -1.94427611e-01 -9.15249179e-01  5.55706284e-01\n",
      "  -1.33517940e+00  8.87010748e-01 -1.37463230e+00 -2.17419647e+00\n",
      "  -1.25740135e+00 -1.31318055e+00 -6.14154079e-01  2.22614888e-01\n",
      "  -1.46933551e+00 -1.68493354e-01  1.13868634e+00  1.89460041e-01]\n",
      " [-1.43467485e+00 -4.55907479e-01  7.61920907e-01 -6.70436295e-01\n",
      "   4.56099269e-01  9.78955865e-01 -8.22718403e-01  8.08045316e-01\n",
      "   6.27021632e-01 -1.38991139e+00  1.66149739e+00 -8.56711947e-01\n",
      "  -2.41747192e+00  8.04668145e-01  3.51728369e-01 -1.05110033e-01\n",
      "   1.21352374e+00 -1.09328995e+00  2.43180525e-01  1.59171295e+00\n",
      "  -1.26667614e+00 -3.73449091e-01  4.63176103e+00 -7.50170875e-01\n",
      "   4.08422045e-01 -2.34879114e-01  4.93355206e-02  3.53774184e+00\n",
      "   2.60078936e-01  3.11223894e-01 -1.33014864e+00 -1.05431019e+00\n",
      "   6.89786229e-01  4.70521389e-01 -5.20654377e-02 -2.58222305e+00\n",
      "   7.70521150e-01 -5.23787272e-01  4.01843062e-01  9.39692913e-02\n",
      "   1.55464403e+00 -1.37450524e-01  9.99508586e-01  7.82297252e-01\n",
      "   6.00719992e-02 -1.16845032e+00 -6.29914694e-01 -1.83286491e+00]\n",
      " [-1.21208060e+00  8.51322157e-01  7.32024074e-01 -9.91218677e-01\n",
      "  -4.03526361e-01 -9.14554398e-01  6.99791961e-01  1.08192306e+00\n",
      "   2.10599488e-01 -6.97421014e-01  6.62362410e-01  4.11232606e-01\n",
      "  -5.65406874e-02 -1.21826628e+00  1.72252537e+00 -5.50962487e-01\n",
      "  -9.71961349e-02 -5.51861506e-01 -4.19431466e-03  3.82168768e-01\n",
      "  -1.05487890e+00  9.48010188e-02 -2.00411950e+00 -4.76317695e-01\n",
      "   2.11947143e+00 -2.52728467e-01 -1.18054554e+00 -1.44893787e+00\n",
      "  -2.07702856e-01 -1.38802521e+00 -3.59274190e-01  1.94179620e+00\n",
      "  -1.21244052e+00 -7.75144803e-01 -9.52184685e-01  1.15406788e+00\n",
      "  -1.15509538e+00 -2.45674323e+00 -2.04755536e-01 -2.48896541e+00\n",
      "   1.28602180e+00  1.19585223e+00 -5.07059585e-01  2.99515990e-01\n",
      "   1.38718886e+00 -6.22638861e-01  1.25825557e+00 -7.09389291e-01]]\n"
     ]
    }
   ],
   "source": [
    "mse_vec = (errors**2).mean(axis=1)\n",
    "median_mse = np.median(mse_vec)\n",
    "mean_mse = mse_vec.mean()\n",
    "print(\"Median MSE is {}\".format(median_mse))\n",
    "print(\"Mean MSE is {}\".format(mean_mse))\n",
    "print(\"MSE is {}\".format(mse_vec))\n",
    "\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot erros of the first run of the simulation\n",
    "plt.plot((predictions[0,:]))\n",
    "plt.title(\"Predicted values of the first run of the simulation\")\n",
    "# Include tick at every other integer\n",
    "plt.xticks(np.arange(0, 50, 2))\n",
    "plt.show()\n",
    "\n",
    "# Plot the histogram of  mean squared error over the different runs\n",
    "plt.hist(mse_vec, bins=40)\n",
    "plt.title(\"Histogram of mean squared error over the different runs\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
