{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "from helpers.functions import winsor, corr_uniform, sPCAest\n",
    "from scipy.ndimage import shift\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA, KernelPCA, SparsePCA, FastICA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weak factor case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250,)\n",
      "(250,)\n",
      "(250, 500)\n",
      "y_t:  [ 2.02458128 -0.01216413 -2.38703033  1.8379149  -1.3016181  -2.62430784\n",
      "  0.84799199  1.4169033   1.36392416  1.4309837 ]\n",
      "y_h:  [-0.01216413 -2.38703033  1.8379149  -1.3016181  -2.62430784  0.84799199\n",
      "  1.4169033   1.36392416  1.4309837  -2.18787295]\n",
      "\n",
      "e:  [-0.17314292 -0.8747935   0.90478288 -2.00206665 -1.01499776 -0.34406969\n",
      "  0.7162563   0.57286603  1.22382556 -2.16874685]\n",
      "g:  [ 0.16097879 -1.51223682  0.93313202  0.70044855 -1.60931008  1.19206167\n",
      "  0.70064699  0.79105813  0.20715815 -0.0191261 ]\n"
     ]
    }
   ],
   "source": [
    "def generate_data(n, T, N, h_steps=1, heteroskedastic=False, psi_max=None, rho=None):\n",
    "    # Check if weak or strong factor model\n",
    "    if n == N:\n",
    "        # Strong factor model, require psi_max and rho\n",
    "        assert psi_max is not None\n",
    "        assert rho is not None\n",
    "\n",
    "    # Generate y_{t + h} = g_t + e_{t + h}\n",
    "    # e_{t + h} ~ N(0, 1)\n",
    "    # g_t ~ N(0, 1)\n",
    "    # X_t,i = g_t*phi_i + h_t*psi_i + u_t,i\n",
    "\n",
    "    # Set T to T + h to account for lag\n",
    "    T_plus_h = T + h_steps\n",
    "\n",
    "    # Generate g_t\n",
    "    g = np.zeros(T_plus_h)\n",
    "    g = np.random.normal(0, 1, T_plus_h)\n",
    "\n",
    "    # Generate h_t\n",
    "    h = np.zeros(T_plus_h)\n",
    "    h = np.random.normal(0, 1, T_plus_h)\n",
    "    \n",
    "    # Generate e_t+h\n",
    "    e = np.zeros(T_plus_h)\n",
    "    e = np.random.normal(0, 1, T_plus_h)\n",
    "\n",
    "    # Generate y_{t+h}\n",
    "    y_h = np.zeros(T_plus_h)\n",
    "    y_h = g + e\n",
    "\n",
    "    # Generate y_t\n",
    "    y_t = np.zeros(T_plus_h)\n",
    "    y_t[h_steps:T_plus_h] = y_h[:T]\n",
    "\n",
    "    phi = np.zeros(N)\n",
    "    psi = np.zeros(N)\n",
    "\n",
    "    if n < N:\n",
    "        # Generate phi_i (Nx1) with n < N nonzero elements\n",
    "        phi[:n] = np.random.uniform(0, 1, n)\n",
    "\n",
    "        # Generate psi_i (Nx1) with n < N nonzero elements\n",
    "        psi[:n] = np.random.uniform(0, 1, n)\n",
    "\n",
    "        # Generate idiosyncratic error's variances\n",
    "        variance_u = np.random.uniform(0, 1, N)        \n",
    "    elif n == N:\n",
    "        # Phi is U(0, phi_max)\n",
    "        psi = np.random.uniform(0, psi_max, N)\n",
    "\n",
    "        # Draw correlated phi and sigma_u\n",
    "        # Generate correlation matrix for phi and sigma_u\n",
    "        phi, variance_u = corr_uniform(rho=rho*1.1, size=N)\n",
    "\n",
    "    u = np.zeros((T, N))\n",
    "    # Generate u_t,i depending on whether the model is heteroskedastic or not\n",
    "    if heteroskedastic:\n",
    "        for t in range(T):\n",
    "            scale = np.random.uniform(0.5, 1.5)\n",
    "            u[t, :] = np.random.normal(0, variance_u * scale, N)\n",
    "    else:\n",
    "        # Generate u\n",
    "        u = np.random.multivariate_normal(mean=np.zeros(N), cov=np.diag(variance_u), size=T)\n",
    "\n",
    "    # Drop first h rows\n",
    "    y_h = y_h[h_steps:]\n",
    "    y_t = y_t[h_steps:]\n",
    "    e = e[h_steps:]\n",
    "    g = g[h_steps:]\n",
    "    h = h[h_steps:]\n",
    "\n",
    "    X = g[:, np.newaxis]*phi[np.newaxis, :] + h[:, np.newaxis]*psi[np.newaxis, :] + u\n",
    "\n",
    "    results = {'y_t':y_t,\n",
    "               'y_h':y_h,\n",
    "                'X':X,\n",
    "                'g':g,\n",
    "                'e':e,\n",
    "                }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Testing\n",
    "results = generate_data(10, T=250, N=500, h_steps=1)\n",
    "y_t = results['y_t']\n",
    "y_h = results['y_h']\n",
    "X = results['X']\n",
    "e = results['e']\n",
    "g = results['g']\n",
    "\n",
    "print(y_t.shape)\n",
    "print(y_h.shape)\n",
    "print(X.shape)\n",
    "\n",
    "print('y_t: ', y_t[:10])\n",
    "print('y_h: ', y_h[:10])\n",
    "print()\n",
    "print('e: ', e[:10])\n",
    "print('g: ', g[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pca(X, y_h, y_t, h_steps=1, nfac=2, method=\"sPCA\", start_test=200):\n",
    "    pca = PCA(n_components=nfac)\n",
    "    reg = LinearRegression()\n",
    "\n",
    "    predicted = np.zeros((1, 50 - h_steps - 1))\n",
    "    error_mat = np.zeros((1, 50 - h_steps - 1))\n",
    "    normalize = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "    for t in range(50 - h_steps - 1):\n",
    "        if t % 10 == 0:\n",
    "            print(\"Period {}\".format(t))\n",
    "\n",
    "        # Split data into training and testing sets\n",
    "        # Training set is the first 200 + t observations\n",
    "        # Testing set is the last 50 - t observations\n",
    "        idx_split = start_test + t\n",
    "        \n",
    "        X_train = X[:idx_split]\n",
    "        y_train = y_h[:idx_split]\n",
    "        y_test = y_t[idx_split]\n",
    "\n",
    "        if method == \"sPCA\":\n",
    "            # Estimate the parameters of the model using sPCAest\n",
    "            # The function sPCAest is defined in functions.py\n",
    "            factors, _ = sPCAest(y_train, X_train, nfac,[0, 90], h_steps)\n",
    "        elif method == \"PCA\":\n",
    "            X_normalized = normalize.fit_transform(X_train)\n",
    "            factors = pca.fit_transform(X_normalized)\n",
    "\n",
    "        reg.fit(factors[:-h_steps,:], y_train[:-h_steps])\n",
    "        \n",
    "        # Predict y_{t+h} using the estimated parameters\n",
    "        y_pred = reg.predict(factors[-1].reshape(1, -1))\n",
    "\n",
    "        # Add predicted value to the predicted vector\n",
    "        predicted[0, t] = y_pred\n",
    "        \n",
    "        # Compute the error\n",
    "        error_mat[0, t] = y_pred - y_test\n",
    "\n",
    "    return error_mat, predicted\n",
    "\n",
    "\n",
    "def simulate_PCA(n=10, T=250, N=500, h_steps=1, R=10, nfac=2, method=\"sPCA\", heteroskedastic=False, psi_max=None, rho=None):\n",
    "    # Initialize the MSE vector\n",
    "    predicted = np.zeros((R, 50 - h_steps - 1))\n",
    "    error_mat = np.zeros((R, 50 - h_steps - 1))\n",
    "\n",
    "    start_test = 200\n",
    "\n",
    "    for r in range(R):\n",
    "        print('r: ', r)\n",
    "        variables = generate_data(n=n, T=T, h_steps=h_steps, N=N, heteroskedastic=heteroskedastic, psi_max=psi_max, rho=rho)\n",
    "\n",
    "        # y_t contains the values of y at time t, and y_h contains the values of y at time t+h\n",
    "        # X contains the values of X at time t\n",
    "        y_h = variables['y_h']\n",
    "        y_t = variables['y_t']\n",
    "        X = variables['X']\n",
    "        \n",
    "        # Loop for expanding window estimation\n",
    "        # The initial window size is 200 observations\n",
    "        # The window size is increased by 1 observation at each iteration\n",
    "        # The window size is increased until it reaches 250 observations\n",
    "        errors, predictions = predict_pca(X, y_h, y_t, h_steps, nfac, method, start_test)\n",
    "        error_mat[r, :] = errors\n",
    "        predicted[r, :] = predictions\n",
    "        \n",
    "    return error_mat, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r:  0\n",
      "Period 0\n",
      "Period 10\n",
      "Period 20\n",
      "Period 30\n",
      "Period 40\n",
      "r:  1\n",
      "Period 0\n",
      "Period 10\n",
      "Period 20\n",
      "Period 30\n",
      "Period 40\n",
      "r:  2\n",
      "Period 0\n",
      "Period 10\n",
      "Period 20\n",
      "Period 30\n",
      "Period 40\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Run the simulationc:\\Users\\Vincent\\Anaconda3New\\envs\\thesis\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:729\n",
    "errors, predictions = simulate_PCA(n=50, T=250, N=500, h_steps=1, R=3, nfac=2, method=\"PCA\", heteroskedastic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median MSE is 1.2032784403170285\n",
      "Mean MSE is 1.3454188785284196\n",
      "MSE is [1.1533483  1.67962989 1.20327844]\n",
      "[[-1.37698031e+00 -1.51825559e+00 -2.45048694e+00 -7.93658656e-02\n",
      "   5.95462830e-01 -2.33476740e+00 -7.59228655e-01 -4.68956948e-01\n",
      "   9.11057507e-01  5.46488803e-01  1.80725337e-01  1.29629386e+00\n",
      "   2.11389521e-01  3.95513080e-01  2.95997324e-01 -1.49515763e+00\n",
      "   1.76824381e+00  1.69391660e+00  1.10807763e+00  1.11321922e+00\n",
      "   6.67677012e-02 -6.46077988e-01  8.01870294e-02  5.23966656e-01\n",
      "   1.35655611e+00  6.63764057e-01 -5.32275170e-01  9.97696500e-02\n",
      "  -5.45598588e-01  1.41257072e+00  2.94573106e-02 -3.51512564e-01\n",
      "  -1.18330142e+00 -1.94427611e-01 -9.15249179e-01  5.55706284e-01\n",
      "  -1.33517940e+00  8.87010748e-01 -1.37463230e+00 -2.17419647e+00\n",
      "  -1.25740135e+00 -1.31318055e+00 -6.14154079e-01  2.22614888e-01\n",
      "  -1.46933551e+00 -1.68493354e-01  1.13868634e+00  1.89460041e-01]\n",
      " [-1.43467485e+00 -4.55907479e-01  7.61920907e-01 -6.70436295e-01\n",
      "   4.56099269e-01  9.78955865e-01 -8.22718403e-01  8.08045316e-01\n",
      "   6.27021632e-01 -1.38991139e+00  1.66149739e+00 -8.56711947e-01\n",
      "  -2.41747192e+00  8.04668145e-01  3.51728369e-01 -1.05110033e-01\n",
      "   1.21352374e+00 -1.09328995e+00  2.43180525e-01  1.59171295e+00\n",
      "  -1.26667614e+00 -3.73449091e-01  4.63176103e+00 -7.50170875e-01\n",
      "   4.08422045e-01 -2.34879114e-01  4.93355206e-02  3.53774184e+00\n",
      "   2.60078936e-01  3.11223894e-01 -1.33014864e+00 -1.05431019e+00\n",
      "   6.89786229e-01  4.70521389e-01 -5.20654377e-02 -2.58222305e+00\n",
      "   7.70521150e-01 -5.23787272e-01  4.01843062e-01  9.39692913e-02\n",
      "   1.55464403e+00 -1.37450524e-01  9.99508586e-01  7.82297252e-01\n",
      "   6.00719992e-02 -1.16845032e+00 -6.29914694e-01 -1.83286491e+00]\n",
      " [-1.21208060e+00  8.51322157e-01  7.32024074e-01 -9.91218677e-01\n",
      "  -4.03526361e-01 -9.14554398e-01  6.99791961e-01  1.08192306e+00\n",
      "   2.10599488e-01 -6.97421014e-01  6.62362410e-01  4.11232606e-01\n",
      "  -5.65406874e-02 -1.21826628e+00  1.72252537e+00 -5.50962487e-01\n",
      "  -9.71961349e-02 -5.51861506e-01 -4.19431466e-03  3.82168768e-01\n",
      "  -1.05487890e+00  9.48010188e-02 -2.00411950e+00 -4.76317695e-01\n",
      "   2.11947143e+00 -2.52728467e-01 -1.18054554e+00 -1.44893787e+00\n",
      "  -2.07702856e-01 -1.38802521e+00 -3.59274190e-01  1.94179620e+00\n",
      "  -1.21244052e+00 -7.75144803e-01 -9.52184685e-01  1.15406788e+00\n",
      "  -1.15509538e+00 -2.45674323e+00 -2.04755536e-01 -2.48896541e+00\n",
      "   1.28602180e+00  1.19585223e+00 -5.07059585e-01  2.99515990e-01\n",
      "   1.38718886e+00 -6.22638861e-01  1.25825557e+00 -7.09389291e-01]]\n"
     ]
    }
   ],
   "source": [
    "mse_vec = (errors**2).mean(axis=1)\n",
    "median_mse = np.median(mse_vec)\n",
    "mean_mse = mse_vec.mean()\n",
    "print(\"Median MSE is {}\".format(median_mse))\n",
    "print(\"Mean MSE is {}\".format(mean_mse))\n",
    "print(\"MSE is {}\".format(mse_vec))\n",
    "\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot erros of the first run of the simulation\n",
    "plt.plot((predictions[0,:]))\n",
    "plt.title(\"Predicted values of the first run of the simulation\")\n",
    "# Include tick at every other integer\n",
    "plt.xticks(np.arange(0, 50, 2))\n",
    "plt.show()\n",
    "\n",
    "# Plot the histogram of  mean squared error over the different runs\n",
    "plt.hist(mse_vec, bins=40)\n",
    "plt.title(\"Histogram of mean squared error over the different runs\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
